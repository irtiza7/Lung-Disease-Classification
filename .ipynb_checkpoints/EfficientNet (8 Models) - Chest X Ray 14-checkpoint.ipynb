{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3fa4032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping\n",
    "from keras.preprocessing import image                  \n",
    "from sklearn.utils import shuffle\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Flatten, Dense\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import BatchNormalization\n",
    "from keras import regularizers, applications, optimizers, initializers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3, EfficientNetB4, EfficientNetB5, EfficientNetB6, EfficientNetB7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a75c69c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(y_true, y_pred):\n",
    "    return K.mean(K.equal(y_true, K.round(y_pred)))\n",
    "\n",
    "def precision_threshold(threshold = 0.5):\n",
    "    def precision(y_true, y_pred):\n",
    "        threshold_value = threshold\n",
    "        y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), threshold_value), K.floatx())\n",
    "        true_positives = K.round(K.sum(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(y_pred)\n",
    "        precision_ratio = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision_ratio\n",
    "    return precision\n",
    "\n",
    "def recall_threshold(threshold = 0.5):\n",
    "    def recall(y_true, y_pred):\n",
    "        threshold_value = threshold\n",
    "        y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), threshold_value), K.floatx())\n",
    "        true_positives = K.round(K.sum(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.clip(y_true, 0, 1))\n",
    "        recall_ratio = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall_ratio\n",
    "    return recall\n",
    "\n",
    "def fbeta_score_threshold(beta = 1, threshold = 0.5):\n",
    "    def fbeta_score(y_true, y_pred):\n",
    "        threshold_value = threshold\n",
    "        beta_value = beta\n",
    "        p = precision_threshold(threshold_value)(y_true, y_pred)\n",
    "        r = recall_threshold(threshold_value)(y_true, y_pred)\n",
    "        bb = beta_value ** 2\n",
    "        fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "        return fbeta_score\n",
    "    return fbeta_score\n",
    "\n",
    "def calculate_cm(y_true, y_pred):\n",
    "    fp = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    tp = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    fn = np.sum((y_pred == 0) & (y_true == 1))\n",
    "    tn = np.sum((y_pred == 0) & (y_true == 0))\n",
    "    return tp, fp, fn, tn\n",
    "\n",
    "def calculate_recall(tp, fp, fn, tn):\n",
    "    return (tp)/(tp + fn)\n",
    "\n",
    "def calculate_fallout(tp, fp, fn, tn):\n",
    "    return (fp)/(fp + tn)\n",
    "\n",
    "def calculate_fpr_tpr(y_true, y_pred):\n",
    "    tp, fp, fn, tn = calculate_cm(y_true, y_pred)\n",
    "    tpr = calculate_recall(tp, fp, fn, tn)\n",
    "    fpr = calculate_fallout(tp, fp, fn, tn)\n",
    "    return fpr, tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9db4284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = CSVLogger('saved_models/log_pretrained_CNN.csv')\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/pretrainedVGG.best.from_scratch.hdf5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2c15a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases = [\n",
    "    'Cardiomegaly','Emphysema','Effusion',\n",
    "    'Hernia','Nodule','Pneumothorax',\n",
    "    'Atelectasis','Pleural_Thickening',\n",
    "    'Mass','Edema','Consolidation',\n",
    "    'Infiltration','Fibrosis','Pneumonia'\n",
    "]\n",
    "\n",
    "# dataset_df = pd.read_csv('/kaggle/input/data/Data_Entry_2017.csv')\n",
    "dataset_df = pd.read_csv('./dataset_information/Data_Entry_2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d2239a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying One Hot Encoding to Labels\n",
    "for disease in diseases:\n",
    "    dataset_df[disease] = dataset_df['Finding Labels'].apply(lambda x: 1 if disease in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "743b7dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples Found: 112120\n"
     ]
    }
   ],
   "source": [
    "image_labels = dataset_df[diseases].to_numpy()\n",
    "image_paths = {\n",
    "    # os.path.basename(x): x for x in glob(os.path.join('..', 'input', 'data', 'images*', 'images', '*.png'))\n",
    "    os.path.basename(x): x for x in glob(os.path.join('.', 'images', '*.png'))\n",
    "}\n",
    "\n",
    "print(f\"Samples Found: {len(image_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a8c9cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing path to each image against image name in the dataframe\n",
    "dataset_df['Image Path'] = dataset_df['Image Index'].map(image_paths.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ee4a125",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_list = dataset_df['Image Path'].tolist()\n",
    "\n",
    "labelB = (dataset_df[diseases].sum(axis = 1) > 0).tolist()\n",
    "labelB = np.array(labelB, dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af83e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_to_tensor(path, shape):\n",
    "    # Loads RGB image to PIL format\n",
    "    img = image.load_img(path, target_size = shape)\n",
    "    \n",
    "    # Convert PIL image to 3D tensor of specific shape\n",
    "    # and normalizes it by dividing each pixel by 255\n",
    "    normalized_image_tensor = image.img_to_array(img) / 255\n",
    "    \n",
    "    # Convert 3D tensor to 4D tensor with specific shape \n",
    "    # (1, shape, 3) and return it\n",
    "    return np.expand_dims(normalized_image_tensor, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cefbf62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_array(paths, shape):\n",
    "    images_arrays = [read_image_to_tensor(path, shape) for path in tqdm(paths, desc = \"Progress\", ncols = 100)]\n",
    "    return np.vstack(images_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f3b5a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some hyper-parameters\n",
    "IMAGE_SHAPE = (70, 70)\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caa167b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|███████████████████████████████████████████████| 78484/78484 [20:22<00:00, 64.18it/s]\n",
      "Progress: 100%|███████████████████████████████████████████████| 11212/11212 [02:51<00:00, 65.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# Samples in Training Set: 70%\n",
    "# Samples in Validation Set: 10%\n",
    "\n",
    "# Storing labels of samples for each set\n",
    "train_labels = labelB[ : 78484][ : , np.newaxis]\n",
    "valid_labels = labelB[78484 : 89696][ : , np.newaxis]\n",
    "\n",
    "# Storing arrays of samples for each set\n",
    "training_samples = image_to_array(images_list[ : 78484], shape = IMAGE_SHAPE)\n",
    "validation_samples = image_to_array(images_list[78484 : 89696], shape = IMAGE_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a93edfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model with EfficientNet as base.\n",
    "e_net = EfficientNetB0(\n",
    "    weights = 'imagenet',\n",
    "    include_top = False,\n",
    "    input_shape = training_samples.shape[1 : ]\n",
    ")\n",
    "\n",
    "custom_classifier = Sequential()\n",
    "custom_classifier.add(GlobalAveragePooling2D(input_shape = e_net.output_shape[1 : ]))\n",
    "custom_classifier.add(Dropout(0.2))\n",
    "custom_classifier.add(Dense(256, activation = 'relu'))\n",
    "custom_classifier.add(Dropout(0.2))\n",
    "custom_classifier.add(Dense(512, activation = 'relu'))\n",
    "custom_classifier.add(Dropout(0.2))\n",
    "custom_classifier.add(Dense(256, activation = 'relu'))\n",
    "custom_classifier.add(Dropout(0.2))\n",
    "custom_classifier.add(Dense(50, activation = 'relu'))\n",
    "custom_classifier.add(Dropout(0.2))\n",
    "custom_classifier.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model = Model(inputs = e_net.input, outputs = custom_classifier(e_net.output))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9ea5d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining 2 optimizers to test the model with.\n",
    "\n",
    "SGD_optimizer = tf.keras.optimizers.SGD(\n",
    "    learning_rate = 1e-4, \n",
    "    decay = 1e-6, \n",
    "    momentum = 0.9, \n",
    "    nesterov = True\n",
    ")\n",
    "adam_optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate = 0.001,\n",
    "    beta_1 = 0.9,\n",
    "    beta_2 = 0.999,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bc98a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining object for augmentation \n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    featurewise_center = False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center = False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization = False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization = False,  # divide each input by its std\n",
    "    zca_whitening = False,  # apply ZCA whitening\n",
    "    rotation_range = 10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range = 0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range = 0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip = True,  # randomly flip images\n",
    "    vertical_flip = False \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2952d9ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compiling model with loss function, optimizer and metrics\n",
    "\n",
    "model.compile(\n",
    "    optimizer = SGD_optimizer,\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = [\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.FalseNegatives(),\n",
    "        tf.keras.metrics.TrueNegatives(),\n",
    "        tf.keras.metrics.FalsePositives(),\n",
    "        tf.keras.metrics.TrueNegatives(),\n",
    "        precision_threshold(threshold = 0.5), \n",
    "        recall_threshold(threshold = 0.5), \n",
    "        fbeta_score_threshold(beta=0.5, threshold = 0.5)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "525121b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m7irt\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2452/2452 [==============================] - ETA: 0s - loss: 0.6949 - accuracy: 0.5228 - false_negatives: 25551.0000 - true_negatives: 31235.0000 - false_positives: 11889.0000 - true_negatives_1: 31235.0000 - precision: 0.4570 - recall: 0.2771 - fbeta_score: 0.3711WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 350 batches). You may need to use the repeat() function when building your dataset.\n",
      "2452/2452 [==============================] - 341s 132ms/step - loss: 0.6949 - accuracy: 0.5228 - false_negatives: 25551.0000 - true_negatives: 31235.0000 - false_positives: 11889.0000 - true_negatives_1: 31235.0000 - precision: 0.4570 - recall: 0.2771 - fbeta_score: 0.3711 - val_loss: 0.7043 - val_accuracy: 0.4752 - val_false_negatives: 5884.0000 - val_true_negatives: 5328.0000 - val_false_positives: 0.0000e+00 - val_true_negatives_1: 5328.0000 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_fbeta_score: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.70427, saving model to saved_models\\pretrainedVGG.best.from_scratch.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m7irt\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "2452/2452 [==============================] - 331s 135ms/step - loss: 0.6877 - accuracy: 0.5462 - false_negatives: 30012.0000 - true_negatives: 37540.0000 - false_positives: 5587.0000 - true_negatives_1: 37540.0000 - precision: 0.4812 - recall: 0.1502 - fbeta_score: 0.3159\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 3/10\n",
      "2452/2452 [==============================] - 335s 137ms/step - loss: 0.6841 - accuracy: 0.5542 - false_negatives: 29024.0000 - true_negatives: 37177.0000 - false_positives: 5951.0000 - true_negatives_1: 37177.0000 - precision: 0.5166 - recall: 0.1784 - fbeta_score: 0.3556\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 4/10\n",
      "2452/2452 [==============================] - 333s 136ms/step - loss: 0.6797 - accuracy: 0.5648 - false_negatives: 27177.0000 - true_negatives: 36155.0000 - false_positives: 6967.0000 - true_negatives_1: 36155.0000 - precision: 0.5401 - recall: 0.2325 - fbeta_score: 0.4110\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 5/10\n",
      "2452/2452 [==============================] - 333s 136ms/step - loss: 0.6717 - accuracy: 0.5860 - false_negatives: 23868.0000 - true_negatives: 34516.0000 - false_positives: 8611.0000 - true_negatives_1: 34516.0000 - precision: 0.5748 - recall: 0.3266 - fbeta_score: 0.4867\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 6/10\n",
      "2452/2452 [==============================] - 331s 135ms/step - loss: 0.6637 - accuracy: 0.6029 - false_negatives: 20338.0000 - true_negatives: 32311.0000 - false_positives: 10816.0000 - true_negatives_1: 32311.0000 - precision: 0.5818 - recall: 0.4272 - fbeta_score: 0.5346\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 7/10\n",
      "2452/2452 [==============================] - 331s 135ms/step - loss: 0.6608 - accuracy: 0.6089 - false_negatives: 19020.0000 - true_negatives: 31473.0000 - false_positives: 11659.0000 - true_negatives_1: 31473.0000 - precision: 0.5846 - recall: 0.4647 - fbeta_score: 0.5491\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 8/10\n",
      "2452/2452 [==============================] - 333s 136ms/step - loss: 0.6548 - accuracy: 0.6201 - false_negatives: 18034.0000 - true_negatives: 31365.0000 - false_positives: 11766.0000 - true_negatives_1: 31365.0000 - precision: 0.5966 - recall: 0.4935 - fbeta_score: 0.5662\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 9/10\n",
      "2452/2452 [==============================] - 334s 136ms/step - loss: 0.6512 - accuracy: 0.6256 - false_negatives: 17590.0000 - true_negatives: 31343.0000 - false_positives: 11784.0000 - true_negatives_1: 31343.0000 - precision: 0.6021 - recall: 0.5070 - fbeta_score: 0.5741\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 10/10\n",
      "2452/2452 [==============================] - 339s 138ms/step - loss: 0.6481 - accuracy: 0.6326 - false_negatives: 17045.0000 - true_negatives: 31347.0000 - false_positives: 11782.0000 - true_negatives_1: 31347.0000 - precision: 0.6092 - recall: 0.5217 - fbeta_score: 0.5834\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "55min 44s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_datagen.flow(\n",
    "        training_samples, \n",
    "        train_labels, \n",
    "        batch_size = BATCH_SIZE\n",
    "    ),\n",
    "    steps_per_epoch = len(training_samples) // BATCH_SIZE,\n",
    "    validation_data = (validation_samples, valid_labels),\n",
    "    validation_steps = len(validation_samples) // BATCH_SIZE,\n",
    "    epochs = EPOCHS,\n",
    "    callbacks = [checkpointer], \n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "939e4bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1075"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freeing Memory with Python's Garbase Collector\n",
    "del training_samples\n",
    "del validation_samples\n",
    "del train_labels\n",
    "del valid_labels\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c4bad5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|███████████████████████████████████████████████| 22424/22424 [06:04<00:00, 61.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# Samples in Testing Set: 20%\n",
    "test_labels = labelB[89696 : ][ : , np.newaxis]\n",
    "test_samples = image_to_array(images_list[89696 : ], shape = IMAGE_SHAPE)\n",
    "\n",
    "prediction = model.predict(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67f2abc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.46780234575271606 \n",
      "Recall: 1.0 \n",
      "Specificity: 0.0004187955440154117\n",
      "Precision: 0.46768367290496826 \n",
      "F1-Score: 0.5234072804450989\n",
      "\n",
      "True Positives: 10485 \n",
      "False Positives: 11934 \n",
      "False Negatives: 0 \n",
      "True Negatives: 5\n",
      "\n",
      "False Positve Rate: 99.95812044559847 % \n",
      "True Positive Rate: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of Trained Model\n",
    "testing_predictions = []\n",
    "for val in prediction:\n",
    "    if val[0] < 0.5:\n",
    "        testing_predictions.append([0])\n",
    "    else:\n",
    "        testing_predictions.append([1])\n",
    "\n",
    "testing_predictions = np.array(testing_predictions)\n",
    "\n",
    "TP, FP, FN, TN = calculate_cm(test_labels, testing_predictions)\n",
    "FPR, TPR = calculate_fpr_tpr(test_labels, testing_predictions)\n",
    "\n",
    "threshold = 0.5\n",
    "beta = 0.5\n",
    "\n",
    "accuracy = K.eval(binary_accuracy(K.variable(value=test_labels), K.variable(value=prediction)))\n",
    "precision = K.eval(precision_threshold(threshold = threshold)(K.variable(value=test_labels),K.variable(value=prediction)))\n",
    "recall = K.eval(recall_threshold(threshold = threshold)(K.variable(value=test_labels),K.variable(value=prediction)))\n",
    "f1_score = K.eval(fbeta_score_threshold(beta = beta, threshold = threshold)(K.variable(value=test_labels),K.variable(value=prediction)))\n",
    "\n",
    "print (f\"Accuracy: {accuracy} \\nRecall: {recall} \\nSpecificity: {TN / (TN + FP)}\\nPrecision: {precision} \\nF1-Score: {f1_score}\\n\")\n",
    "print (f\"True Positives: {TP} \\nFalse Positives: {FP} \\nFalse Negatives: {FN} \\nTrue Negatives: {TN}\\n\")\n",
    "print (f\"False Positve Rate: {FPR * 100} % \\nTrue Positive Rate: {TPR * 100} %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
