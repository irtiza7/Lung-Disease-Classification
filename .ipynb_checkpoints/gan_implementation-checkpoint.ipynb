{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b4b39e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfa64c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_dim):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(256, img_dim),\n",
    "            nn.Tanh(),  # normalize inputs to [-1, 1] so make outputs [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7c3071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters etc.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 3e-4\n",
    "z_dim = 64\n",
    "image_dim = 28 * 28 * 1  # 784\n",
    "batch_size = 32\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a98dec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = Discriminator(image_dim).to(device)\n",
    "gen = Generator(z_dim, image_dim).to(device)\n",
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device)\n",
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "956fcba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to dataset/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/MNIST\\raw\\train-images-idx3-ubyte.gz to dataset/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to dataset/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/MNIST\\raw\\train-labels-idx1-ubyte.gz to dataset/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to dataset/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/MNIST\\raw\\t10k-images-idx3-ubyte.gz to dataset/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to dataset/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to dataset/MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.MNIST(root=\"dataset/\", transform=transforms, download=True)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf036321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50] Batch 0/1875                       Loss D: 0.5682, loss G: 1.1186\n",
      "Epoch [1/50] Batch 0/1875                       Loss D: 0.6166, loss G: 1.0475\n",
      "Epoch [2/50] Batch 0/1875                       Loss D: 0.6075, loss G: 0.8853\n",
      "Epoch [3/50] Batch 0/1875                       Loss D: 0.6100, loss G: 1.1147\n",
      "Epoch [4/50] Batch 0/1875                       Loss D: 0.6213, loss G: 0.9554\n",
      "Epoch [5/50] Batch 0/1875                       Loss D: 0.6540, loss G: 0.8255\n",
      "Epoch [6/50] Batch 0/1875                       Loss D: 0.6462, loss G: 0.9952\n",
      "Epoch [7/50] Batch 0/1875                       Loss D: 0.6368, loss G: 0.9328\n",
      "Epoch [8/50] Batch 0/1875                       Loss D: 0.5659, loss G: 1.1151\n",
      "Epoch [9/50] Batch 0/1875                       Loss D: 0.6129, loss G: 0.9185\n",
      "Epoch [10/50] Batch 0/1875                       Loss D: 0.6580, loss G: 0.9637\n",
      "Epoch [11/50] Batch 0/1875                       Loss D: 0.6432, loss G: 1.2282\n",
      "Epoch [12/50] Batch 0/1875                       Loss D: 0.6647, loss G: 0.8223\n",
      "Epoch [13/50] Batch 0/1875                       Loss D: 0.6707, loss G: 0.8370\n",
      "Epoch [14/50] Batch 0/1875                       Loss D: 0.6542, loss G: 0.8502\n",
      "Epoch [15/50] Batch 0/1875                       Loss D: 0.6302, loss G: 1.1206\n",
      "Epoch [16/50] Batch 0/1875                       Loss D: 0.5918, loss G: 0.9858\n",
      "Epoch [17/50] Batch 0/1875                       Loss D: 0.6446, loss G: 0.8597\n",
      "Epoch [18/50] Batch 0/1875                       Loss D: 0.6735, loss G: 0.8516\n",
      "Epoch [19/50] Batch 0/1875                       Loss D: 0.5954, loss G: 1.0048\n",
      "Epoch [20/50] Batch 0/1875                       Loss D: 0.5754, loss G: 0.9085\n",
      "Epoch [21/50] Batch 0/1875                       Loss D: 0.6753, loss G: 0.8873\n",
      "Epoch [22/50] Batch 0/1875                       Loss D: 0.4929, loss G: 1.1900\n",
      "Epoch [23/50] Batch 0/1875                       Loss D: 0.5506, loss G: 1.0272\n",
      "Epoch [24/50] Batch 0/1875                       Loss D: 0.6968, loss G: 0.8169\n",
      "Epoch [25/50] Batch 0/1875                       Loss D: 0.5446, loss G: 1.0932\n",
      "Epoch [26/50] Batch 0/1875                       Loss D: 0.6981, loss G: 0.8811\n",
      "Epoch [27/50] Batch 0/1875                       Loss D: 0.5493, loss G: 1.0871\n",
      "Epoch [28/50] Batch 0/1875                       Loss D: 0.4960, loss G: 1.1654\n",
      "Epoch [29/50] Batch 0/1875                       Loss D: 0.5568, loss G: 0.9255\n",
      "Epoch [30/50] Batch 0/1875                       Loss D: 0.6365, loss G: 0.8825\n",
      "Epoch [31/50] Batch 0/1875                       Loss D: 0.5172, loss G: 1.2249\n",
      "Epoch [32/50] Batch 0/1875                       Loss D: 0.5318, loss G: 1.0998\n",
      "Epoch [33/50] Batch 0/1875                       Loss D: 0.6349, loss G: 0.8967\n",
      "Epoch [34/50] Batch 0/1875                       Loss D: 0.6107, loss G: 1.0427\n",
      "Epoch [35/50] Batch 0/1875                       Loss D: 0.5199, loss G: 1.0573\n",
      "Epoch [36/50] Batch 0/1875                       Loss D: 0.6602, loss G: 0.7934\n",
      "Epoch [37/50] Batch 0/1875                       Loss D: 0.6147, loss G: 1.0003\n",
      "Epoch [38/50] Batch 0/1875                       Loss D: 0.6712, loss G: 0.9990\n",
      "Epoch [39/50] Batch 0/1875                       Loss D: 0.7067, loss G: 0.9405\n",
      "Epoch [40/50] Batch 0/1875                       Loss D: 0.6864, loss G: 1.0580\n",
      "Epoch [41/50] Batch 0/1875                       Loss D: 0.5685, loss G: 1.4116\n",
      "Epoch [42/50] Batch 0/1875                       Loss D: 0.6101, loss G: 0.9447\n",
      "Epoch [43/50] Batch 0/1875                       Loss D: 0.5807, loss G: 1.0270\n",
      "Epoch [44/50] Batch 0/1875                       Loss D: 0.7149, loss G: 0.8034\n",
      "Epoch [45/50] Batch 0/1875                       Loss D: 0.5480, loss G: 0.9940\n",
      "Epoch [46/50] Batch 0/1875                       Loss D: 0.5726, loss G: 1.0172\n",
      "Epoch [47/50] Batch 0/1875                       Loss D: 0.6163, loss G: 1.0923\n",
      "Epoch [48/50] Batch 0/1875                       Loss D: 0.6849, loss G: 1.1470\n",
      "Epoch [49/50] Batch 0/1875                       Loss D: 0.5641, loss G: 0.9976\n",
      "Epoch [50/50] Batch 0/1875                       Loss D: 0.5578, loss G: 0.9932\n",
      "Epoch [51/50] Batch 0/1875                       Loss D: 0.5463, loss G: 1.0779\n",
      "Epoch [52/50] Batch 0/1875                       Loss D: 0.5660, loss G: 1.0872\n",
      "Epoch [53/50] Batch 0/1875                       Loss D: 0.6063, loss G: 0.9473\n",
      "Epoch [54/50] Batch 0/1875                       Loss D: 0.6517, loss G: 1.0633\n",
      "Epoch [55/50] Batch 0/1875                       Loss D: 0.6876, loss G: 1.0525\n",
      "Epoch [56/50] Batch 0/1875                       Loss D: 0.5652, loss G: 0.9668\n",
      "Epoch [57/50] Batch 0/1875                       Loss D: 0.6938, loss G: 0.9131\n",
      "Epoch [58/50] Batch 0/1875                       Loss D: 0.6096, loss G: 0.9529\n",
      "Epoch [59/50] Batch 0/1875                       Loss D: 0.6576, loss G: 0.9035\n",
      "Epoch [60/50] Batch 0/1875                       Loss D: 0.4868, loss G: 1.2405\n",
      "Epoch [61/50] Batch 0/1875                       Loss D: 0.6258, loss G: 1.1478\n",
      "Epoch [62/50] Batch 0/1875                       Loss D: 0.5415, loss G: 1.0869\n",
      "Epoch [63/50] Batch 0/1875                       Loss D: 0.6177, loss G: 1.2473\n",
      "Epoch [64/50] Batch 0/1875                       Loss D: 0.5966, loss G: 1.1643\n",
      "Epoch [65/50] Batch 0/1875                       Loss D: 0.5570, loss G: 0.8544\n",
      "Epoch [66/50] Batch 0/1875                       Loss D: 0.5785, loss G: 1.0317\n",
      "Epoch [67/50] Batch 0/1875                       Loss D: 0.7019, loss G: 1.2654\n",
      "Epoch [68/50] Batch 0/1875                       Loss D: 0.5452, loss G: 1.2477\n",
      "Epoch [69/50] Batch 0/1875                       Loss D: 0.5270, loss G: 0.9945\n",
      "Epoch [70/50] Batch 0/1875                       Loss D: 0.6609, loss G: 1.0258\n",
      "Epoch [71/50] Batch 0/1875                       Loss D: 0.5386, loss G: 1.3432\n",
      "Epoch [72/50] Batch 0/1875                       Loss D: 0.5445, loss G: 1.0115\n",
      "Epoch [73/50] Batch 0/1875                       Loss D: 0.5037, loss G: 0.9781\n",
      "Epoch [74/50] Batch 0/1875                       Loss D: 0.6516, loss G: 1.1989\n",
      "Epoch [75/50] Batch 0/1875                       Loss D: 0.6691, loss G: 0.9426\n",
      "Epoch [76/50] Batch 0/1875                       Loss D: 0.6379, loss G: 0.9802\n",
      "Epoch [77/50] Batch 0/1875                       Loss D: 0.6309, loss G: 0.8805\n",
      "Epoch [78/50] Batch 0/1875                       Loss D: 0.5172, loss G: 1.0247\n",
      "Epoch [79/50] Batch 0/1875                       Loss D: 0.6029, loss G: 1.0545\n",
      "Epoch [80/50] Batch 0/1875                       Loss D: 0.5458, loss G: 1.0195\n",
      "Epoch [81/50] Batch 0/1875                       Loss D: 0.6511, loss G: 1.0955\n",
      "Epoch [82/50] Batch 0/1875                       Loss D: 0.6204, loss G: 1.2523\n",
      "Epoch [83/50] Batch 0/1875                       Loss D: 0.5841, loss G: 1.2097\n",
      "Epoch [84/50] Batch 0/1875                       Loss D: 0.5801, loss G: 0.9498\n",
      "Epoch [85/50] Batch 0/1875                       Loss D: 0.6501, loss G: 0.7653\n",
      "Epoch [86/50] Batch 0/1875                       Loss D: 0.6922, loss G: 1.0660\n",
      "Epoch [87/50] Batch 0/1875                       Loss D: 0.5628, loss G: 1.0656\n",
      "Epoch [88/50] Batch 0/1875                       Loss D: 0.6201, loss G: 1.2220\n",
      "Epoch [89/50] Batch 0/1875                       Loss D: 0.5744, loss G: 1.1389\n",
      "Epoch [90/50] Batch 0/1875                       Loss D: 0.6549, loss G: 0.9472\n",
      "Epoch [91/50] Batch 0/1875                       Loss D: 0.6085, loss G: 0.8611\n",
      "Epoch [92/50] Batch 0/1875                       Loss D: 0.5598, loss G: 1.2077\n",
      "Epoch [93/50] Batch 0/1875                       Loss D: 0.5334, loss G: 1.3285\n",
      "Epoch [94/50] Batch 0/1875                       Loss D: 0.5412, loss G: 1.0576\n",
      "Epoch [95/50] Batch 0/1875                       Loss D: 0.6402, loss G: 0.9404\n",
      "Epoch [96/50] Batch 0/1875                       Loss D: 0.5247, loss G: 1.1763\n",
      "Epoch [97/50] Batch 0/1875                       Loss D: 0.6267, loss G: 1.1440\n",
      "Epoch [98/50] Batch 0/1875                       Loss D: 0.5790, loss G: 1.1447\n",
      "Epoch [99/50] Batch 0/1875                       Loss D: 0.5959, loss G: 1.3376\n",
      "Epoch [100/50] Batch 0/1875                       Loss D: 0.5987, loss G: 1.0569\n",
      "Epoch [101/50] Batch 0/1875                       Loss D: 0.5455, loss G: 1.2503\n",
      "Epoch [102/50] Batch 0/1875                       Loss D: 0.6934, loss G: 1.1969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [103/50] Batch 0/1875                       Loss D: 0.6647, loss G: 1.1537\n",
      "Epoch [104/50] Batch 0/1875                       Loss D: 0.6279, loss G: 1.0358\n",
      "Epoch [105/50] Batch 0/1875                       Loss D: 0.5844, loss G: 1.1308\n",
      "Epoch [106/50] Batch 0/1875                       Loss D: 0.6216, loss G: 1.0437\n",
      "Epoch [107/50] Batch 0/1875                       Loss D: 0.5329, loss G: 1.1552\n",
      "Epoch [108/50] Batch 0/1875                       Loss D: 0.6226, loss G: 1.0248\n",
      "Epoch [109/50] Batch 0/1875                       Loss D: 0.5893, loss G: 1.2361\n",
      "Epoch [110/50] Batch 0/1875                       Loss D: 0.5107, loss G: 0.9685\n",
      "Epoch [111/50] Batch 0/1875                       Loss D: 0.5905, loss G: 1.0912\n",
      "Epoch [112/50] Batch 0/1875                       Loss D: 0.5931, loss G: 1.1392\n",
      "Epoch [113/50] Batch 0/1875                       Loss D: 0.6928, loss G: 0.9507\n",
      "Epoch [114/50] Batch 0/1875                       Loss D: 0.5616, loss G: 0.9898\n",
      "Epoch [115/50] Batch 0/1875                       Loss D: 0.5440, loss G: 1.4513\n",
      "Epoch [116/50] Batch 0/1875                       Loss D: 0.6546, loss G: 1.2607\n",
      "Epoch [117/50] Batch 0/1875                       Loss D: 0.6002, loss G: 1.0396\n",
      "Epoch [118/50] Batch 0/1875                       Loss D: 0.4991, loss G: 1.3131\n",
      "Epoch [119/50] Batch 0/1875                       Loss D: 0.5897, loss G: 1.1341\n",
      "Epoch [120/50] Batch 0/1875                       Loss D: 0.5241, loss G: 1.0211\n",
      "Epoch [121/50] Batch 0/1875                       Loss D: 0.5809, loss G: 1.1143\n",
      "Epoch [122/50] Batch 0/1875                       Loss D: 0.5231, loss G: 1.2506\n",
      "Epoch [123/50] Batch 0/1875                       Loss D: 0.6235, loss G: 1.1261\n",
      "Epoch [124/50] Batch 0/1875                       Loss D: 0.6046, loss G: 0.9804\n",
      "Epoch [125/50] Batch 0/1875                       Loss D: 0.5286, loss G: 1.0401\n",
      "Epoch [126/50] Batch 0/1875                       Loss D: 0.4966, loss G: 1.4372\n",
      "Epoch [127/50] Batch 0/1875                       Loss D: 0.5223, loss G: 1.1799\n",
      "Epoch [128/50] Batch 0/1875                       Loss D: 0.5381, loss G: 1.2080\n",
      "Epoch [129/50] Batch 0/1875                       Loss D: 0.4280, loss G: 1.3631\n",
      "Epoch [130/50] Batch 0/1875                       Loss D: 0.5572, loss G: 1.3509\n",
      "Epoch [131/50] Batch 0/1875                       Loss D: 0.5655, loss G: 0.9663\n",
      "Epoch [132/50] Batch 0/1875                       Loss D: 0.4807, loss G: 1.4403\n",
      "Epoch [133/50] Batch 0/1875                       Loss D: 0.5637, loss G: 1.0535\n",
      "Epoch [134/50] Batch 0/1875                       Loss D: 0.6553, loss G: 0.9661\n",
      "Epoch [135/50] Batch 0/1875                       Loss D: 0.4682, loss G: 1.3691\n",
      "Epoch [136/50] Batch 0/1875                       Loss D: 0.5733, loss G: 0.9740\n",
      "Epoch [137/50] Batch 0/1875                       Loss D: 0.5643, loss G: 1.1903\n",
      "Epoch [138/50] Batch 0/1875                       Loss D: 0.4288, loss G: 1.3263\n",
      "Epoch [139/50] Batch 0/1875                       Loss D: 0.5131, loss G: 1.1713\n",
      "Epoch [140/50] Batch 0/1875                       Loss D: 0.6160, loss G: 0.9986\n",
      "Epoch [141/50] Batch 0/1875                       Loss D: 0.6277, loss G: 1.1075\n",
      "Epoch [142/50] Batch 0/1875                       Loss D: 0.5509, loss G: 1.3026\n",
      "Epoch [143/50] Batch 0/1875                       Loss D: 0.4818, loss G: 1.1206\n",
      "Epoch [144/50] Batch 0/1875                       Loss D: 0.5636, loss G: 1.2767\n",
      "Epoch [145/50] Batch 0/1875                       Loss D: 0.6038, loss G: 1.2750\n",
      "Epoch [146/50] Batch 0/1875                       Loss D: 0.6119, loss G: 1.1178\n",
      "Epoch [147/50] Batch 0/1875                       Loss D: 0.5556, loss G: 1.2388\n",
      "Epoch [148/50] Batch 0/1875                       Loss D: 0.5021, loss G: 1.1316\n",
      "Epoch [149/50] Batch 0/1875                       Loss D: 0.5715, loss G: 1.0873\n",
      "Epoch [150/50] Batch 0/1875                       Loss D: 0.5590, loss G: 0.9857\n",
      "Epoch [151/50] Batch 0/1875                       Loss D: 0.5391, loss G: 1.4135\n",
      "Epoch [152/50] Batch 0/1875                       Loss D: 0.6042, loss G: 1.0799\n",
      "Epoch [153/50] Batch 0/1875                       Loss D: 0.5515, loss G: 1.1919\n",
      "Epoch [154/50] Batch 0/1875                       Loss D: 0.4986, loss G: 1.1590\n",
      "Epoch [155/50] Batch 0/1875                       Loss D: 0.5102, loss G: 1.1511\n",
      "Epoch [156/50] Batch 0/1875                       Loss D: 0.5560, loss G: 1.1360\n",
      "Epoch [157/50] Batch 0/1875                       Loss D: 0.5917, loss G: 1.2693\n",
      "Epoch [158/50] Batch 0/1875                       Loss D: 0.5687, loss G: 0.9726\n",
      "Epoch [159/50] Batch 0/1875                       Loss D: 0.5065, loss G: 1.0954\n",
      "Epoch [160/50] Batch 0/1875                       Loss D: 0.6046, loss G: 1.2111\n",
      "Epoch [161/50] Batch 0/1875                       Loss D: 0.5553, loss G: 1.1577\n",
      "Epoch [162/50] Batch 0/1875                       Loss D: 0.5567, loss G: 1.0913\n",
      "Epoch [163/50] Batch 0/1875                       Loss D: 0.5907, loss G: 1.0474\n",
      "Epoch [164/50] Batch 0/1875                       Loss D: 0.5403, loss G: 1.3986\n",
      "Epoch [165/50] Batch 0/1875                       Loss D: 0.5549, loss G: 1.2443\n",
      "Epoch [166/50] Batch 0/1875                       Loss D: 0.5753, loss G: 1.3269\n",
      "Epoch [167/50] Batch 0/1875                       Loss D: 0.5399, loss G: 1.2036\n",
      "Epoch [168/50] Batch 0/1875                       Loss D: 0.5392, loss G: 1.2252\n",
      "Epoch [169/50] Batch 0/1875                       Loss D: 0.5102, loss G: 1.1435\n",
      "Epoch [170/50] Batch 0/1875                       Loss D: 0.4683, loss G: 1.2980\n",
      "Epoch [171/50] Batch 0/1875                       Loss D: 0.5211, loss G: 1.2999\n",
      "Epoch [172/50] Batch 0/1875                       Loss D: 0.6013, loss G: 1.3123\n",
      "Epoch [173/50] Batch 0/1875                       Loss D: 0.4851, loss G: 1.1977\n",
      "Epoch [174/50] Batch 0/1875                       Loss D: 0.6080, loss G: 1.1451\n",
      "Epoch [175/50] Batch 0/1875                       Loss D: 0.5554, loss G: 1.1330\n",
      "Epoch [176/50] Batch 0/1875                       Loss D: 0.4916, loss G: 1.2317\n",
      "Epoch [177/50] Batch 0/1875                       Loss D: 0.5254, loss G: 1.4302\n",
      "Epoch [178/50] Batch 0/1875                       Loss D: 0.6397, loss G: 0.9220\n",
      "Epoch [179/50] Batch 0/1875                       Loss D: 0.5386, loss G: 1.1866\n",
      "Epoch [180/50] Batch 0/1875                       Loss D: 0.6699, loss G: 1.1358\n",
      "Epoch [181/50] Batch 0/1875                       Loss D: 0.4817, loss G: 1.0612\n",
      "Epoch [182/50] Batch 0/1875                       Loss D: 0.5654, loss G: 0.8892\n",
      "Epoch [183/50] Batch 0/1875                       Loss D: 0.5999, loss G: 1.0656\n",
      "Epoch [184/50] Batch 0/1875                       Loss D: 0.5979, loss G: 1.2146\n",
      "Epoch [185/50] Batch 0/1875                       Loss D: 0.5979, loss G: 1.2352\n",
      "Epoch [186/50] Batch 0/1875                       Loss D: 0.5676, loss G: 1.0084\n",
      "Epoch [187/50] Batch 0/1875                       Loss D: 0.5610, loss G: 1.1514\n",
      "Epoch [188/50] Batch 0/1875                       Loss D: 0.4608, loss G: 1.3874\n",
      "Epoch [189/50] Batch 0/1875                       Loss D: 0.5507, loss G: 1.2853\n",
      "Epoch [190/50] Batch 0/1875                       Loss D: 0.5546, loss G: 1.1044\n",
      "Epoch [191/50] Batch 0/1875                       Loss D: 0.5981, loss G: 1.1878\n",
      "Epoch [192/50] Batch 0/1875                       Loss D: 0.4317, loss G: 1.5739\n",
      "Epoch [193/50] Batch 0/1875                       Loss D: 0.5183, loss G: 1.1394\n",
      "Epoch [194/50] Batch 0/1875                       Loss D: 0.5492, loss G: 1.0354\n",
      "Epoch [195/50] Batch 0/1875                       Loss D: 0.5806, loss G: 1.1035\n",
      "Epoch [196/50] Batch 0/1875                       Loss D: 0.6317, loss G: 1.5134\n",
      "Epoch [197/50] Batch 0/1875                       Loss D: 0.4645, loss G: 1.2631\n",
      "Epoch [198/50] Batch 0/1875                       Loss D: 0.5368, loss G: 1.3045\n",
      "Epoch [199/50] Batch 0/1875                       Loss D: 0.5646, loss G: 1.0490\n",
      "Epoch [200/50] Batch 0/1875                       Loss D: 0.6272, loss G: 1.1408\n",
      "Epoch [201/50] Batch 0/1875                       Loss D: 0.6635, loss G: 1.1229\n",
      "Epoch [202/50] Batch 0/1875                       Loss D: 0.4151, loss G: 1.3663\n",
      "Epoch [203/50] Batch 0/1875                       Loss D: 0.5290, loss G: 1.0808\n",
      "Epoch [204/50] Batch 0/1875                       Loss D: 0.5244, loss G: 1.1431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [205/50] Batch 0/1875                       Loss D: 0.5070, loss G: 1.4701\n",
      "Epoch [206/50] Batch 0/1875                       Loss D: 0.5513, loss G: 1.5055\n",
      "Epoch [207/50] Batch 0/1875                       Loss D: 0.5483, loss G: 1.1767\n",
      "Epoch [208/50] Batch 0/1875                       Loss D: 0.4852, loss G: 1.3499\n",
      "Epoch [209/50] Batch 0/1875                       Loss D: 0.4915, loss G: 1.3954\n",
      "Epoch [210/50] Batch 0/1875                       Loss D: 0.5486, loss G: 1.1847\n",
      "Epoch [211/50] Batch 0/1875                       Loss D: 0.4716, loss G: 1.2016\n",
      "Epoch [212/50] Batch 0/1875                       Loss D: 0.5215, loss G: 1.1130\n",
      "Epoch [213/50] Batch 0/1875                       Loss D: 0.5973, loss G: 1.0911\n",
      "Epoch [214/50] Batch 0/1875                       Loss D: 0.4900, loss G: 1.1104\n",
      "Epoch [215/50] Batch 0/1875                       Loss D: 0.5218, loss G: 1.3368\n",
      "Epoch [216/50] Batch 0/1875                       Loss D: 0.5865, loss G: 1.2826\n",
      "Epoch [217/50] Batch 0/1875                       Loss D: 0.5680, loss G: 1.2440\n",
      "Epoch [218/50] Batch 0/1875                       Loss D: 0.5036, loss G: 1.6142\n",
      "Epoch [219/50] Batch 0/1875                       Loss D: 0.5942, loss G: 1.0346\n",
      "Epoch [220/50] Batch 0/1875                       Loss D: 0.5434, loss G: 1.2554\n",
      "Epoch [221/50] Batch 0/1875                       Loss D: 0.5348, loss G: 1.1672\n",
      "Epoch [222/50] Batch 0/1875                       Loss D: 0.5179, loss G: 1.4429\n",
      "Epoch [223/50] Batch 0/1875                       Loss D: 0.5881, loss G: 1.0954\n",
      "Epoch [224/50] Batch 0/1875                       Loss D: 0.5213, loss G: 1.1216\n",
      "Epoch [225/50] Batch 0/1875                       Loss D: 0.5937, loss G: 1.2083\n",
      "Epoch [226/50] Batch 0/1875                       Loss D: 0.4793, loss G: 1.1251\n",
      "Epoch [227/50] Batch 0/1875                       Loss D: 0.5578, loss G: 1.3223\n",
      "Epoch [228/50] Batch 0/1875                       Loss D: 0.5811, loss G: 1.2611\n",
      "Epoch [229/50] Batch 0/1875                       Loss D: 0.5479, loss G: 1.2993\n",
      "Epoch [230/50] Batch 0/1875                       Loss D: 0.6512, loss G: 0.8056\n",
      "Epoch [231/50] Batch 0/1875                       Loss D: 0.5784, loss G: 0.8989\n",
      "Epoch [232/50] Batch 0/1875                       Loss D: 0.5528, loss G: 1.0600\n",
      "Epoch [233/50] Batch 0/1875                       Loss D: 0.5486, loss G: 1.2651\n",
      "Epoch [234/50] Batch 0/1875                       Loss D: 0.4877, loss G: 1.2627\n",
      "Epoch [235/50] Batch 0/1875                       Loss D: 0.5863, loss G: 0.8228\n",
      "Epoch [236/50] Batch 0/1875                       Loss D: 0.5113, loss G: 1.2873\n",
      "Epoch [237/50] Batch 0/1875                       Loss D: 0.4792, loss G: 1.4191\n",
      "Epoch [238/50] Batch 0/1875                       Loss D: 0.5330, loss G: 1.4860\n",
      "Epoch [239/50] Batch 0/1875                       Loss D: 0.6553, loss G: 0.9798\n",
      "Epoch [240/50] Batch 0/1875                       Loss D: 0.5531, loss G: 1.1322\n",
      "Epoch [241/50] Batch 0/1875                       Loss D: 0.6205, loss G: 0.9053\n",
      "Epoch [242/50] Batch 0/1875                       Loss D: 0.4828, loss G: 1.3843\n",
      "Epoch [243/50] Batch 0/1875                       Loss D: 0.4860, loss G: 1.5356\n",
      "Epoch [244/50] Batch 0/1875                       Loss D: 0.5112, loss G: 1.7363\n",
      "Epoch [245/50] Batch 0/1875                       Loss D: 0.5839, loss G: 1.1223\n",
      "Epoch [246/50] Batch 0/1875                       Loss D: 0.4744, loss G: 1.4836\n",
      "Epoch [247/50] Batch 0/1875                       Loss D: 0.5198, loss G: 1.2115\n",
      "Epoch [248/50] Batch 0/1875                       Loss D: 0.5860, loss G: 1.0664\n",
      "Epoch [249/50] Batch 0/1875                       Loss D: 0.5531, loss G: 1.1171\n",
      "Epoch [250/50] Batch 0/1875                       Loss D: 0.5698, loss G: 1.0772\n",
      "Epoch [251/50] Batch 0/1875                       Loss D: 0.5780, loss G: 1.6161\n",
      "Epoch [252/50] Batch 0/1875                       Loss D: 0.4287, loss G: 1.7229\n",
      "Epoch [253/50] Batch 0/1875                       Loss D: 0.5063, loss G: 1.3232\n",
      "Epoch [254/50] Batch 0/1875                       Loss D: 0.6262, loss G: 1.1719\n",
      "Epoch [255/50] Batch 0/1875                       Loss D: 0.4806, loss G: 1.4863\n",
      "Epoch [256/50] Batch 0/1875                       Loss D: 0.4802, loss G: 1.5162\n",
      "Epoch [257/50] Batch 0/1875                       Loss D: 0.5314, loss G: 1.2575\n",
      "Epoch [258/50] Batch 0/1875                       Loss D: 0.6011, loss G: 1.1887\n",
      "Epoch [259/50] Batch 0/1875                       Loss D: 0.5577, loss G: 1.1317\n",
      "Epoch [260/50] Batch 0/1875                       Loss D: 0.4902, loss G: 1.3800\n",
      "Epoch [261/50] Batch 0/1875                       Loss D: 0.4662, loss G: 1.3413\n",
      "Epoch [262/50] Batch 0/1875                       Loss D: 0.4812, loss G: 1.7982\n",
      "Epoch [263/50] Batch 0/1875                       Loss D: 0.5396, loss G: 1.2301\n",
      "Epoch [264/50] Batch 0/1875                       Loss D: 0.4504, loss G: 1.6479\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(500):\n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        real = real.view(-1, 784).to(device)\n",
    "        batch_size = real.shape[0]\n",
    "\n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        noise = torch.randn(batch_size, z_dim).to(device)\n",
    "        fake = gen(noise)\n",
    "        disc_real = disc(real).view(-1)\n",
    "        lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake).view(-1)\n",
    "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        lossD = (lossD_real + lossD_fake) / 2\n",
    "        disc.zero_grad()\n",
    "        lossD.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        # where the second option of maximizing doesn't suffer from\n",
    "        # saturating gradients\n",
    "        output = disc(fake).view(-1)\n",
    "        lossG = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                      Loss D: {lossD:.4f}, loss G: {lossG:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise).reshape(-1, 1, 28, 28)\n",
    "                data = real.reshape(-1, 1, 28, 28)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "                img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "\n",
    "                writer_fake.add_image(\n",
    "                    \"Mnist Fake Images\", img_grid_fake, global_step=step\n",
    "                )\n",
    "                writer_real.add_image(\n",
    "                    \"Mnist Real Images\", img_grid_real, global_step=step\n",
    "                )\n",
    "                step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "077ba576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "020070d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
