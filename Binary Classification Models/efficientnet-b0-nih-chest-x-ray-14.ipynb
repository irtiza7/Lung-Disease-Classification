{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom tqdm import tqdm\nfrom sklearn.utils import shuffle\n\ndf = pd.read_csv('../input/data/Data_Entry_2017.csv')\n\ndiseases = ['Cardiomegaly','Emphysema','Effusion','Hernia','Nodule','Pneumothorax','Atelectasis','Pleural_Thickening','Mass','Edema','Consolidation','Infiltration','Fibrosis','Pneumonia']\n#Number diseases\nfor disease in diseases :\n    df[disease] = df['Finding Labels'].apply(lambda x: 1 if disease in x else 0)","metadata":{"execution":{"iopub.status.busy":"2022-11-20T22:01:07.071998Z","iopub.execute_input":"2022-11-20T22:01:07.072968Z","iopub.status.idle":"2022-11-20T22:01:09.335599Z","shell.execute_reply.started":"2022-11-20T22:01:07.072920Z","shell.execute_reply":"2022-11-20T22:01:09.334713Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import os\nlabels = df[diseases].to_numpy()\nall_image_paths = {os.path.basename(x): x for x in \n                   glob(os.path.join('..', 'input','data','images*','images','*.png'))}\nprint('Images found:', len(all_image_paths))\n\n\ndf['Path'] = df['Image Index'].map(all_image_paths.get)\nfiles_list = df['Path'].tolist()\n\n# #test to perfect\n# labelB = df['Emphysema'].tolist()\n\nlabelB = (df[diseases].sum(axis=1)>0).tolist()\nlabelB = np.array(labelB, dtype=int)","metadata":{"execution":{"iopub.status.busy":"2022-11-20T22:01:09.337339Z","iopub.execute_input":"2022-11-20T22:01:09.337848Z","iopub.status.idle":"2022-11-20T22:01:12.630355Z","shell.execute_reply.started":"2022-11-20T22:01:09.337811Z","shell.execute_reply":"2022-11-20T22:01:12.629371Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Images found: 112120\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.preprocessing import image                  \nfrom tqdm import tqdm\n\ndef path_to_tensor(img_path, shape):\n    # loads RGB image as PIL.Image.Image type\n    img = image.load_img(img_path, target_size=shape)\n    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n    x = image.img_to_array(img)/255\n    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n    return np.expand_dims(x, axis=0)\n\ndef paths_to_tensor(img_paths, shape):\n    list_of_tensors = [path_to_tensor(img_path, shape) for img_path in tqdm(img_paths)]\n    return np.vstack(list_of_tensors)\n\ntrain_labels = labelB[:89600][:, np.newaxis]\nvalid_labels = labelB[89600:100800][:, np.newaxis]\ntest_labels = labelB[100800:][:, np.newaxis]\n\n\nimg_shape = (128, 128)\n# train_tensors = paths_to_tensor(files_list[:89600], shape = img_shape)\n# valid_tensors = paths_to_tensor(files_list[89600:100800], shape = img_shape)\n# test_tensors = paths_to_tensor(files_list[100800:], shape = img_shape)\ntrain_tensors = paths_to_tensor(files_list[:40000], shape = img_shape)\nvalid_tensors = paths_to_tensor(files_list[40000:60000], shape = img_shape)\ntest_tensors = paths_to_tensor(files_list[60000:80000], shape = img_shape)","metadata":{"execution":{"iopub.status.busy":"2022-11-20T22:01:12.631911Z","iopub.execute_input":"2022-11-20T22:01:12.632481Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"100%|██████████| 40000/40000 [17:10<00:00, 38.83it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import time\n\nfrom keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Flatten, Dense\nfrom keras.models import Sequential, Model\nfrom keras.layers import BatchNormalization\nfrom keras import regularizers, applications, optimizers, initializers\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import EfficientNetB0\n\nbase_model = EfficientNetB0(\n    weights = 'imagenet', \n    include_top = False,\n    input_shape = (128, 128, 3)\n#     input_shape = train_tensors.shape[1:]\n)\n\nadd_model = Sequential()\nadd_model.add(GlobalAveragePooling2D(input_shape=base_model.output_shape[1:]))\nadd_model.add(Dropout(0.2))\nadd_model.add(Dense(256, activation='relu'))\nadd_model.add(Dropout(0.2))\nadd_model.add(Dense(512, activation='relu'))\nadd_model.add(Dropout(0.2))\nadd_model.add(Dense(15, activation='softmax'))\n\nmodel = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n# model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import backend as K\n\ndef binary_accuracy(y_true, y_pred):\n    return K.mean(K.equal(y_true, K.round(y_pred)))\n\ndef precision_threshold(threshold = 0.5):\n    def precision(y_true, y_pred):\n        threshold_value = threshold\n        y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), threshold_value), K.floatx())\n        true_positives = K.round(K.sum(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(y_pred)\n        precision_ratio = true_positives / (predicted_positives + K.epsilon())\n        return precision_ratio\n    return precision\n\ndef recall_threshold(threshold = 0.5):\n    def recall(y_true, y_pred):\n        threshold_value = threshold\n        y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), threshold_value), K.floatx())\n        true_positives = K.round(K.sum(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.clip(y_true, 0, 1))\n        recall_ratio = true_positives / (possible_positives + K.epsilon())\n        return recall_ratio\n    return recall\n\ndef fbeta_score_threshold(beta = 1, threshold = 0.5):\n    def fbeta_score(y_true, y_pred):\n        threshold_value = threshold\n        beta_value = beta\n        p = precision_threshold(threshold_value)(y_true, y_pred)\n        r = recall_threshold(threshold_value)(y_true, y_pred)\n        bb = beta_value ** 2\n        fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n        return fbeta_score\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras.backend as K\nimport tensorflow as tf\n\nmodel.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1e-4, decay=1e-6, momentum=0.9, nesterov=True), \n              loss='categorical_crossentropy', \n              metrics=['accuracy',\n                      precision_threshold(threshold = 0.5), \n                       recall_threshold(threshold = 0.5), \n                       fbeta_score_threshold(beta=0.5, threshold = 0.5)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping\nimport numpy as np\n\nepochs = 20\nbatch_size = 32\n\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=4, verbose=1, mode='auto')\nlog = CSVLogger('saved_models/log_pretrained_CNN.csv')\ncheckpointer = ModelCheckpoint(filepath='saved_models/pretrainedVGG.best.from_scratch.hdf5', verbose=1, save_best_only=True)\n\nstart = time.time()\ntrain_datagen = ImageDataGenerator(\n                        featurewise_center=False,  # set input mean to 0 over the dataset\n                        samplewise_center=False,  # set each sample mean to 0\n                        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n                        samplewise_std_normalization=False,  # divide each input by its std\n                        zca_whitening=False,  # apply ZCA whitening\n                        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n                        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n                        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n                        horizontal_flip=True,  # randomly flip images\n                        vertical_flip=False \n)\n\n\n\n# Training with data augmentation. If shift_fraction=0., also no augmentation.\nhistory = model.fit_generator(\n    train_datagen.flow(train_tensors,train_labels, batch_size = batch_size),\n    steps_per_epoch = len(train_tensors) // batch_size,\n    validation_data = (valid_tensors, valid_labels),\n    validation_steps = len(valid_tensors) // batch_size,\n    epochs = epochs,\n#     callbacks=[checkpointer, log], \n    verbose=1\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Training time: %.2f minutes\"%((time.time()-start)/60))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = model.predict(test_tensors)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt  \n\nplt.figure(1, figsize = (15,8))  \n    \nplt.subplot(222)  \nplt.plot(history.history['loss'])  \nplt.plot(history.history['val_loss'])  \nplt.title('efficient-net model loss')  \nplt.ylabel('loss')  \nplt.xlabel('epoch')  \nplt.legend(['train', 'valid']) \n\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt  \n\nplt.figure(1, figsize = (15,8))  \n    \nplt.subplot(222)  \nplt.plot(history.history['accuracy'])  \nplt.plot(history.history['val_accuracy'])  \nplt.title('efficient-net model accuracy')  \nplt.ylabel('loss')  \nplt.xlabel('epoch')  \nplt.legend(['train', 'valid']) \n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['recall'])\nplt.plot(history.history['val_recall'])\n\nplt.title('efficient-net model recall')\nplt.ylabel('recall')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['precision'])\nplt.plot(history.history['val_precision'])\n\nplt.title('efficient-net model precision')\nplt.ylabel('f1_score')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshold = 0.5\nbeta = 0.5\n\npre = K.eval(\n    precision_threshold(threshold = threshold)(K.variable(value=test_labels),\n                                               K.variable(value=prediction)))\nrec = K.eval(\n    recall_threshold(threshold = threshold)(K.variable(value=test_labels),\n                                   K.variable(value=prediction)))\n# fsc = K.eval(\n#     fbeta_score_threshold(beta = beta, threshold = threshold)(K.variable(value=test_labels),\n#                                   K.variable(value=prediction)))\n\nprint (\"Precision: %f %%\\nRecall: %f \"% (pre, rec))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_score = 2 * ((pre * rec) / (pre + rec + K.epsilon()))\nprint(f\"F1 Score: {f1_score}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"K.eval(binary_accuracy(K.variable(value=test_labels), K.variable(value=prediction)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}